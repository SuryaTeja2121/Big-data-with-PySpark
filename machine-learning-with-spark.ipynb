{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a tutorial of machine learning with PySpark. I will create a classification model and a regression model using Pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sparkmagic\n",
    "#!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Spark SQL and Spark ML Libraries\n",
    "\n",
    "We'll train a **LogisticRegression** model with a **Pipleline** preparing the data, a **CrossValidator** to tuene the parameters of the model, and a **BinaryClassificationEvaluator** to evaluate our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "#from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, VectorIndexer, MinMaxScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Source Data\n",
    "The data from the flight.csv file data includes specific characteristics (or features) for each flight, as well as a column indicating how many minutes late or early the flight arrived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "|DayofMonth|DayOfWeek|Carrier|OriginAirportID|DestAirportID|DepDelay|ArrDelay|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "|        19|        5|     DL|          11433|        13303|      -3|       1|\n",
      "|        19|        5|     DL|          14869|        12478|       0|      -8|\n",
      "|        19|        5|     DL|          14057|        14869|      -4|     -15|\n",
      "|        19|        5|     DL|          15016|        11433|      28|      24|\n",
      "|        19|        5|     DL|          11193|        12892|      -6|     -11|\n",
      "|        19|        5|     DL|          10397|        15016|      -1|     -19|\n",
      "|        19|        5|     DL|          15016|        10397|       0|      -1|\n",
      "|        19|        5|     DL|          10397|        14869|      15|      24|\n",
      "|        19|        5|     DL|          10397|        10423|      33|      34|\n",
      "|        19|        5|     DL|          11278|        10397|     323|     322|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv = spark.read.csv(r'D:\\Subjects\\ARTIFICIAL INTELLIGENCE\\SEMESTER - 4\\Big Data Analytics\\End Sem Project\\flights.csv', inferSchema=True, header=True)\n",
    "csv.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Data for a Classification Model (Decision Tree Learning Model)\n",
    "I select a subset of columns to use as features and create a Boolean label field named *label* with values 1 or 0. Specifically, **1** for flight that arrived late, **0** for flight was early or on-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+---------------+-------------+--------+-----+\n",
      "|DayofMonth|DayOfWeek|Carrier|OriginAirportID|DestAirportID|DepDelay|label|\n",
      "+----------+---------+-------+---------------+-------------+--------+-----+\n",
      "|        19|        5|     DL|          11433|        13303|      -3|    0|\n",
      "|        19|        5|     DL|          14869|        12478|       0|    0|\n",
      "|        19|        5|     DL|          14057|        14869|      -4|    0|\n",
      "|        19|        5|     DL|          15016|        11433|      28|    1|\n",
      "|        19|        5|     DL|          11193|        12892|      -6|    0|\n",
      "|        19|        5|     DL|          10397|        15016|      -1|    0|\n",
      "|        19|        5|     DL|          15016|        10397|       0|    0|\n",
      "|        19|        5|     DL|          10397|        14869|      15|    1|\n",
      "|        19|        5|     DL|          10397|        10423|      33|    1|\n",
      "|        19|        5|     DL|          11278|        10397|     323|    1|\n",
      "+----------+---------+-------+---------------+-------------+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = csv.select(\"DayofMonth\", \"DayOfWeek\", \"Carrier\", \"OriginAirportID\", \"DestAirportID\", \"DepDelay\", ((col(\"ArrDelay\") > 15).cast(\"Int\").alias(\"label\")))\n",
    "data.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Data\n",
    "\n",
    "I will use 70% of the data for training, and reserve 30% for testing. In the testing data, the *label* column is renamed to *trueLabel* so I can use it later to compare predicted labels with known actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Rows: 1890195  Testing Rows: 812023\n"
     ]
    }
   ],
   "source": [
    "splits = data.randomSplit([0.7, 0.3])\n",
    "train = splits[0]\n",
    "test = splits[1].withColumnRenamed(\"label\", \"trueLabel\")\n",
    "train_rows = train.count()\n",
    "test_rows = test.count()\n",
    "print(\"Training Rows:\", train_rows, \" Testing Rows:\", test_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Pipeline\n",
    "\n",
    "A pipeline consists of a series of transformer and estimator stages that typically prepare a DataFrame for modeling and then train a predictive model. In this case, you will create a pipeline with seven stages:\n",
    "* A **StringIndexer estimator** that converts string values to indexes for categorical features\n",
    "* A **VectorAssembler** that combines categorical features into a single vector\n",
    "* A **VectorIndexer** that creates indexes for a vector of categorical features\n",
    "* A **VectorAssembler** that creates a vector of continuous numeric features\n",
    "* A **MinMaxScaler** that normalizes continuous numeric features\n",
    "* A **VectorAssembler** that creates a vector of categorical and continuous features\n",
    "* A **DecisionTreeClassifier** that trains a classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "strIdx = StringIndexer(inputCol = \"Carrier\", outputCol = \"CarrierIdx\")\n",
    "catVect = VectorAssembler(inputCols = [\"CarrierIdx\", \"DayofMonth\", \"DayOfWeek\", \"OriginAirportID\", \"DestAirportID\"], outputCol=\"catFeatures\")\n",
    "catIdx = VectorIndexer(inputCol = catVect.getOutputCol(), outputCol = \"idxCatFeatures\")\n",
    "numVect = VectorAssembler(inputCols = [\"DepDelay\"], outputCol=\"numFeatures\")\n",
    "minMax = MinMaxScaler(inputCol = numVect.getOutputCol(), outputCol=\"normFeatures\")\n",
    "featVect = VectorAssembler(inputCols=[\"idxCatFeatures\", \"normFeatures\"], outputCol=\"features\")\n",
    "lr = LogisticRegression(labelCol=\"label\",featuresCol=\"features\",maxIter=10,regParam=0.3)\n",
    "#dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "pipeline = Pipeline(stages=[strIdx, catVect, catIdx, numVect, minMax, featVect, lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Pipeline to train a model\n",
    "Run the pipeline as an Estimator on the training data to train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "piplineModel = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate label predictions\n",
    "Transform the test data with all of the stages and the trained model in the pipeline to generate label predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------+----------+---------+\n",
      "|features                                           |prediction|trueLabel|\n",
      "+---------------------------------------------------+----------+---------+\n",
      "|[10.0,1.0,0.0,10397.0,10693.0,0.03167185877466251] |0.0       |0        |\n",
      "|[10.0,1.0,0.0,10397.0,12264.0,0.030114226375908618]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,10397.0,12264.0,0.04205607476635514] |0.0       |0        |\n",
      "|[10.0,1.0,0.0,10397.0,13851.0,0.03167185877466251] |0.0       |0        |\n",
      "|[10.0,1.0,0.0,10397.0,13851.0,0.03374870197300104] |0.0       |0        |\n",
      "|[10.0,1.0,0.0,10529.0,11193.0,0.030114226375908618]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,10721.0,11193.0,0.030114226375908618]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,10721.0,13931.0,0.03271028037383177] |0.0       |1        |\n",
      "|[10.0,1.0,0.0,10792.0,11433.0,0.029595015576323987]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,10792.0,12478.0,0.055036344755970926]|0.0       |1        |\n",
      "|[10.0,1.0,0.0,10821.0,11193.0,0.05659397715472482] |0.0       |1        |\n",
      "|[10.0,1.0,0.0,10821.0,12478.0,0.027518172377985463]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11042.0,11433.0,0.028556593977154723]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11057.0,11193.0,0.028037383177570093]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11066.0,13487.0,0.030114226375908618]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11066.0,13487.0,0.030633437175493248]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11193.0,10529.0,0.029595015576323987]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11193.0,10529.0,0.03167185877466251] |0.0       |0        |\n",
      "|[10.0,1.0,0.0,11193.0,10821.0,0.030633437175493248]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11193.0,10821.0,0.03426791277258567] |0.0       |0        |\n",
      "|[10.0,1.0,0.0,11193.0,11057.0,0.033229491173416406]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11193.0,11433.0,0.029075804776739357]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11193.0,11433.0,0.030633437175493248]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11193.0,11433.0,0.03167185877466251] |0.0       |0        |\n",
      "|[10.0,1.0,0.0,11193.0,11618.0,0.030114226375908618]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11193.0,11618.0,0.041536863966770504]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11193.0,11618.0,0.06438213914849429] |0.0       |1        |\n",
      "|[10.0,1.0,0.0,11193.0,12451.0,0.11163032191069575] |1.0       |1        |\n",
      "|[10.0,1.0,0.0,11193.0,13198.0,0.027518172377985463]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11193.0,13487.0,0.029075804776739357]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11193.0,13487.0,0.029595015576323987]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11193.0,13487.0,0.13499480789200416] |1.0       |1        |\n",
      "|[10.0,1.0,0.0,11193.0,13930.0,0.030114226375908618]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11193.0,14100.0,0.029595015576323987]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11193.0,14492.0,0.030114226375908618]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11193.0,14492.0,0.04620976116303219] |0.0       |1        |\n",
      "|[10.0,1.0,0.0,11193.0,15016.0,0.03115264797507788] |0.0       |0        |\n",
      "|[10.0,1.0,0.0,11298.0,12478.0,0.030114226375908618]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11298.0,13487.0,0.030114226375908618]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11433.0,10423.0,0.029595015576323987]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11433.0,10529.0,0.03374870197300104] |0.0       |0        |\n",
      "|[10.0,1.0,0.0,11433.0,10792.0,0.030633437175493248]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11433.0,11042.0,0.03167185877466251] |0.0       |0        |\n",
      "|[10.0,1.0,0.0,11433.0,11042.0,0.0347871235721703]  |0.0       |0        |\n",
      "|[10.0,1.0,0.0,11433.0,11193.0,0.030633437175493248]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11433.0,12264.0,0.03167185877466251] |0.0       |0        |\n",
      "|[10.0,1.0,0.0,11433.0,12339.0,0.03686396677050883] |0.0       |1        |\n",
      "|[10.0,1.0,0.0,11433.0,13198.0,0.030114226375908618]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11433.0,13232.0,0.030633437175493248]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11433.0,13232.0,0.03167185877466251] |0.0       |0        |\n",
      "|[10.0,1.0,0.0,11433.0,13342.0,0.0347871235721703]  |0.0       |0        |\n",
      "|[10.0,1.0,0.0,11433.0,13871.0,0.030633437175493248]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11433.0,13930.0,0.036344755970924195]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11433.0,14492.0,0.030633437175493248]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11618.0,11193.0,0.030114226375908618]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,11618.0,11193.0,0.030114226375908618]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,12191.0,10397.0,0.029595015576323987]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,12191.0,10397.0,0.030633437175493248]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,12264.0,10397.0,0.029075804776739357]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,12266.0,13244.0,0.030633437175493248]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,12266.0,13487.0,0.0430944963655244]  |0.0       |0        |\n",
      "|[10.0,1.0,0.0,12339.0,11433.0,0.029595015576323987]|0.0       |1        |\n",
      "|[10.0,1.0,0.0,12339.0,12478.0,0.030114226375908618]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,12339.0,13244.0,0.030114226375908618]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,12339.0,13487.0,0.030633437175493248]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,12339.0,13487.0,0.037383177570093455]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,12339.0,14492.0,0.028556593977154723]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,12451.0,12478.0,0.0960539979231568]  |1.0       |1        |\n",
      "|[10.0,1.0,0.0,12478.0,10693.0,0.030114226375908618]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,12478.0,10721.0,0.030114226375908618]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,12478.0,10721.0,0.033229491173416406]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,12478.0,10792.0,0.03374870197300104] |0.0       |0        |\n",
      "|[10.0,1.0,0.0,12478.0,11193.0,0.028037383177570093]|0.0       |1        |\n",
      "|[10.0,1.0,0.0,12478.0,11278.0,0.030633437175493248]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,12478.0,11278.0,0.03167185877466251] |0.0       |0        |\n",
      "|[10.0,1.0,0.0,12478.0,12264.0,0.041536863966770504]|0.0       |1        |\n",
      "|[10.0,1.0,0.0,12478.0,12339.0,0.03271028037383177] |0.0       |0        |\n",
      "|[10.0,1.0,0.0,12478.0,12451.0,0.04361370716510903] |0.0       |0        |\n",
      "|[10.0,1.0,0.0,12478.0,13342.0,0.04101765316718588] |0.0       |1        |\n",
      "|[10.0,1.0,0.0,12478.0,13487.0,0.032191069574247146]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,12478.0,13930.0,0.028037383177570093]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,12478.0,13931.0,0.025441329179646935]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,12478.0,13931.0,0.030114226375908618]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,12478.0,14100.0,0.03271028037383177] |0.0       |1        |\n",
      "|[10.0,1.0,0.0,12478.0,14122.0,0.0264797507788162]  |0.0       |0        |\n",
      "|[10.0,1.0,0.0,12478.0,14122.0,0.04465212876427829] |0.0       |1        |\n",
      "|[10.0,1.0,0.0,12953.0,14730.0,0.03115264797507788] |0.0       |0        |\n",
      "|[10.0,1.0,0.0,13198.0,11193.0,0.04361370716510903] |0.0       |0        |\n",
      "|[10.0,1.0,0.0,13232.0,13487.0,0.030633437175493248]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,13244.0,11193.0,0.03271028037383177] |0.0       |0        |\n",
      "|[10.0,1.0,0.0,13244.0,13851.0,0.029595015576323987]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,13244.0,14122.0,0.030633437175493248]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,13244.0,14683.0,0.18016614745586707] |1.0       |1        |\n",
      "|[10.0,1.0,0.0,13487.0,10423.0,0.03271028037383177] |0.0       |0        |\n",
      "|[10.0,1.0,0.0,13487.0,10821.0,0.032191069574247146]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,13487.0,11042.0,0.030114226375908618]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,13487.0,11042.0,0.030114226375908618]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,13487.0,11066.0,0.030633437175493248]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,13487.0,11066.0,0.08307372793354101] |0.0       |1        |\n",
      "|[10.0,1.0,0.0,13487.0,11193.0,0.029075804776739357]|0.0       |0        |\n",
      "+---------------------------------------------------+----------+---------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = piplineModel.transform(test)\n",
    "predicted = prediction.select(\"features\", \"prediction\", \"trueLabel\")\n",
    "predicted.show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking into the results, some trueLabel 1s are predicted as 0. Let's evaluate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating a Classification Model\n",
    "We'll calculate a *Confusion Matrix* and the *Area Under ROC* (Receiver Operating Characteristic) to evaluate the model. \n",
    "### Compute Confusion Matrix\n",
    "Classifiers are typically evaluated by creating a *confusion matrix*, which indicates the number of:\n",
    "- True Positives\n",
    "- True Negatives\n",
    "- False Positives\n",
    "- False Negatives\n",
    "\n",
    "From these core measures, other evaluation metrics such as *precision*, *recall* and *F1* can be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|   metric|             value|\n",
      "+---------+------------------+\n",
      "|       TP|           18797.0|\n",
      "|       FP|              77.0|\n",
      "|       TN|          650073.0|\n",
      "|       FN|          143076.0|\n",
      "| Accuracy|0.8237081954575178|\n",
      "|Precision|0.9959203136590018|\n",
      "+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tp = float(predicted.filter(\"prediction == 1.0 AND truelabel == 1\").count())\n",
    "fp = float(predicted.filter(\"prediction == 1.0 AND truelabel == 0\").count())\n",
    "tn = float(predicted.filter(\"prediction == 0.0 AND truelabel == 0\").count())\n",
    "fn = float(predicted.filter(\"prediction == 0.0 AND truelabel == 1\").count())\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "pr = tp / (tp + fp)\n",
    "re = tp / (tp + fn)\n",
    "metrics = spark.createDataFrame([\n",
    " (\"TP\", tp),\n",
    " (\"FP\", fp),\n",
    " (\"TN\", tn),\n",
    " (\"FN\", fn),\n",
    " (\"Accuracy\", accuracy),\n",
    " (\"Precision\", pr),\n",
    " ],[\"metric\", \"value\"])\n",
    "metrics.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------ END -----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
